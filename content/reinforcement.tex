\subsection{Reinforcement Learning}

A \textbf{Markov decision process (MDP)} is defined by

a set of states  $s\in S$ a set of actions $a \in A$;

Action dependent transition probabilities $T(s,a,s')=P(s'|s, a)$ , so that for each state  s  and action  a ,  $\displaystyle \sum _{s'\in S} T(s,a,s')=1$.

Reward functions $R(s, a, s')$ representing the reward for starting in state  $s$ , taking action  $a$  and ending up in state  $s'$  after one step. (The reward function may also depend only on  $s$ , or only  $s$  and  $a$ .)

Therefore a Markov decision process is defined by $MDP = <S,A,T,R>$ MDPs satisfy the Markov property in that the transition probabilities and rewards depend only on the current state and action, and remain unchanged regardless of the history (i.e. past states and actions) that leads to the current state.

Rewards collected after the  $n$th  step do not depend on the previous states $s_1,s_2,\cdots,s_{n-1}$

\textbf{Markov properties:}\\

Rewards collected after the $n$th  step do not depend on the previous actions $a_1,a_2,\cdots,a_{n}$

\textbf{(Infinite horizon) discounted reward based utility}\\

\begin{align*}
U[s_0,s_1,\ldots ]= R(s_0) + \gamma R(s_1) + \gamma ^2 R(s_2) \ldots = \\
= \sum _{t=0}^\infty \gamma ^ t R(s_ t) \text {where }0\leq \gamma <1\\
\leq \frac{R_{max}}{1-\gamma}
\end{align*}

\textbf{Bellman Equations}\\

value fn $V^{*}(s)$ --  is the expected reward from starting at state $s$ and
acting optimally, i.e. following the optimal policy;\\
\begin{enumerate}
  \item \textbf{value fn} $V^{*}(s)$ – is the expected reward from starting at state $s$ and
acting optimally, i.e. following the optimal policy;
  \item \textbf{Q-function} $Q^{*}(s,a)$ –is the expected reward from starting at state $s$, then
acting with action $a$(not necessarily the optimal action), and acting optimally
afterwards.
\end{enumerate}

$V^{*}(s) = \underset {a}{max}Q^{*}(s,a)$\\

$Q^{*}(s,a)=\sum _{s'}T(s,a,s')[R(s,a,s')+\gamma V^{*}(s')]$\\

Q-value: $Q(s,a)$ in state $s$ take action $a$ and act optimally afterwards. 

Policy $\pi^*: s \rightarrow a$ is set of actions to maximize the expected reward for every state $s$.

\begin{align*}
&\pi^*(s) = argmax_a(Q^*(s,a))\\
&Q^*(s,a)= \sum_{s'} T(s,a,s')[R(s,a,s') + \gamma max_{a}Q(s',a')]
\end{align*}

To find the policy two algos: Value iteration and Q-value iteration (look online).

\textbf{Bellman optimality equation}
$Q^*(s,a)=E[R_{t+1} + \gamma \underset{a}{\max{Q^*(s,a)}}]$

\textbf{Q-value Iteration Update Rule}\\
$Q_{k+1}^*(s, a) = \sum _{s'} T(s, a, s')(R(s, a, s') + \gamma \text {max}_{a'} Q_ k^*(s', a')).$



\subsection{Q value iteration by sampling}

Exponential running average is defined as\\
$\bar{x}_n = (1-\alpha)*\bar{x}_{n-1} + \alpha x_n$\\
\begin{align*}
&Q_{i+1}(s, a) = (1-\alpha)\cdot Q_{i}(s, a) + \alpha \cdot sample_i(s,a)\\
&Q_{i+1}(s, a) = Q_{i}(s, a) + \alpha (Q_{i}(s, a) - sample_i(s,a))\\
&Q_{i+1}(s, a) = Q_{i}(s, a) + \alpha (Q_{i}(s, a) - (R(s,a,s')
+ \gamma max_{a}Q(s',a')))
\end{align*}\\
$Q_{i}(s, a)$ is not the Q-value of the sample $i$, but the average Q-value up
to the sample $i$ including.  It already embed all the experience up to
sample $i$.\\
The above equation should recall those of the stochastic gradient descent,
it’s just a different form of this update.





